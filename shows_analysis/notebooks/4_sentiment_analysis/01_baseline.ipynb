{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c851a7-3419-49b4-b115-1909dd0e1adf",
   "metadata": {},
   "source": [
    "# Baseline: Logistic Regression + TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36631f4-d7e9-4c48-92fe-f2f388a3b324",
   "metadata": {},
   "source": [
    "In this notebook I'm going to create a strong baseline using classical algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b93071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install skl2onnx==1.12.0 onnxruntime==1.13.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3bab6-76e5-4c1a-981e-4f1006c86224",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f9128d-93fe-4eb8-a623-291490a5c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59e6c20d-f2ed-45eb-9918-bdb8f3bac462",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc8f767-3a51-460f-869d-985d5027c7e5",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3fa7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODELS_PATH = \"saved_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31ea325-b4e9-4ddc-9004-028b09f079ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = os.path.join(\"../../../\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "589f2efd-9f1e-492c-b27a-c81c6120839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_data_path = os.path.join(relative_path, \"3_sentiment_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea04001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(SAVED_MODELS_PATH).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc6afa4-81c2-4116-95b4-1c1ede52c871",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259da1b8-158b-4a7c-ae33-4c1b377b8da2",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b68f1853-797e-45d9-afbe-23cd1ab035c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../data/3_sentiment_analysis/split_reviews.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reviews \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_parquet(\n\u001b[1;32m      2\u001b[0m     os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(sentiment_analysis_data_path, \u001b[39m\"\u001b[39;49m\u001b[39msplit_reviews.parquet\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m reviews\u001b[39m.\u001b[39minfo()\n",
      "File \u001b[0;32m~/projects/shows_analysis/venv/lib/python3.8/site-packages/pandas/io/parquet.py:493\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[39mDataFrame\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    491\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[0;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    494\u001b[0m     path,\n\u001b[1;32m    495\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m    496\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    497\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[1;32m    498\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    499\u001b[0m )\n",
      "File \u001b[0;32m~/projects/shows_analysis/venv/lib/python3.8/site-packages/pandas/io/parquet.py:233\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    231\u001b[0m     to_pandas_kwargs[\u001b[39m\"\u001b[39m\u001b[39msplit_blocks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m path_or_handle, handles, kwargs[\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    234\u001b[0m     path,\n\u001b[1;32m    235\u001b[0m     kwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mfilesystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    236\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    237\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    238\u001b[0m )\n\u001b[1;32m    239\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mread_table(\n\u001b[1;32m    241\u001b[0m         path_or_handle, columns\u001b[39m=\u001b[39mcolumns, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    242\u001b[0m     )\u001b[39m.\u001b[39mto_pandas(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mto_pandas_kwargs)\n",
      "File \u001b[0;32m~/projects/shows_analysis/venv/lib/python3.8/site-packages/pandas/io/parquet.py:102\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m     92\u001b[0m handles \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m     94\u001b[0m     \u001b[39mnot\u001b[39;00m fs\n\u001b[1;32m     95\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[39m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     handles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m    103\u001b[0m         path_or_handle, mode, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[1;32m    104\u001b[0m     )\n\u001b[1;32m    105\u001b[0m     fs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     path_or_handle \u001b[39m=\u001b[39m handles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/projects/shows_analysis/venv/lib/python3.8/site-packages/pandas/io/common.py:798\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    799\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[1;32m    801\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../data/3_sentiment_analysis/split_reviews.parquet'"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_parquet(\n",
    "    os.path.join(sentiment_analysis_data_path, \"split_reviews.parquet\")\n",
    ")\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1aa80db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reviews[reviews[\"fold\"] == \"train\"]\n",
    "test = reviews[reviews[\"fold\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b253462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryblo\\AppData\\Local\\Temp\\ipykernel_15432\\3246664530.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"review\"] = test[\"review\"].str.replace(\"<p>\", \" \")\n",
      "C:\\Users\\ryblo\\AppData\\Local\\Temp\\ipykernel_15432\\3246664530.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"review\"] = train[\"review\"].str.replace(\"<p>\", \" \")\n"
     ]
    }
   ],
   "source": [
    "test[\"review\"] = test[\"review\"].str.replace(\"<p>\", \" \")\n",
    "train[\"review\"] = train[\"review\"].str.replace(\"<p>\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dfcc292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186063, 20674, 186063, 20674)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = (\n",
    "    train[\"review\"].values.tolist(),\n",
    "    test[\"review\"].values.tolist(),\n",
    "    train[\"sentiment\"].values.tolist(),\n",
    "    test[\"sentiment\"].values.tolist(),\n",
    ")\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d27c0-9c0a-4dca-8847-93b29d42b7e5",
   "metadata": {},
   "source": [
    "# Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b2255-efab-4def-b111-b485b81ef8f4",
   "metadata": {},
   "source": [
    "### Text encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4840cd-b11e-49b6-a34a-c0d324a72e5a",
   "metadata": {},
   "source": [
    "For baseline model, I've decided to start with TF-IDF and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d28c25b-8866-4ddb-a356-b120c40cb390",
   "metadata": {},
   "source": [
    "#### Hyperparameter Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a9e7d-f32a-40ad-af53-d97386dc7bc4",
   "metadata": {},
   "source": [
    "##### `lowercase`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b415f-c116-4c86-8e6a-75ce62415c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 785942) with lowercase turned off\n",
      "CPU times: total: 40.1 s\n",
      "Wall time: 40.1 s\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=False)\n",
    "vectors_wo_lowercase = vectorizer.fit_transform(train[\"review\"])\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors_wo_lowercase.shape} with lowercase turned off\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7bcc1d-6c1b-4d35-837f-6d4ec1736002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 669383) with lowercase turned on\n",
      "CPU times: total: 41.8 s\n",
      "Wall time: 41.8 s\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectors_w_lowercase = vectorizer.fit_transform(train[\"review\"])\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors_w_lowercase.shape} with lowercase turned on\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac53777-9510-4639-ac9e-c4d48980bf9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116559"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_wo_lowercase.shape[1] - vectors_w_lowercase.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654009f-512d-4374-a222-c41041c716f8",
   "metadata": {},
   "source": [
    "The difference in vocabulary size without making all characters lowercase and with lowercase is more than 100 000, so we better stick to lowercase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b432626-0df6-4b84-a4cf-74068b462185",
   "metadata": {},
   "source": [
    "##### `max_df` and `min_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0025cfd-90bc-4dff-b7d6-a298ae3710f4",
   "metadata": {},
   "source": [
    "`min_df` is used for removing terms that appear **too infrequently**. For example:\n",
    "\n",
    " - `min_df = 0.01` means \"ignore terms that appear in **less than 1% of the documents**\".\n",
    " - `min_df = 5` means \"ignore terms that appear in **less than 5 documents**\".  \n",
    " \n",
    "The default `min_df` is `1`, which means \"ignore terms that appear in **less than 1 document**\".  \n",
    "Thus, the default setting does not ignore any terms.\n",
    "\n",
    "`max_df` is used for removing terms that appear **too frequently**, also known as \"corpus-specific stop words\". For example:\n",
    "\n",
    " - `max_df = 0.50` means \"ignore terms that appear in **more than 50% of the documents**\".\n",
    " - `max_df = 25` means \"ignore terms that appear in **more than 25 documents**\".  \n",
    " \n",
    "The default `max_df` is `1.0`, which means \"ignore terms that appear in **more than 100% of the documents**\".  \n",
    "Thus, the default setting does not ignore any terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9639aed-f80f-4623-95fa-17cfbb9a6459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000', '00000', '000000',\n",
       "       '000000000000000000попкорн000000000000', '000000000000001',\n",
       "       '000000000000на', '00000000000во', '00000000000данной',\n",
       "       '00000000000есть000000000000000',\n",
       "       '00000000000есть000000000000000000', '0000000000жевать',\n",
       "       '0000000000ненавижу00000000', '00000000016', '000000000надо',\n",
       "       '000000000разговаривать0000000000', '00000000визуальная',\n",
       "       '00000001', '000001', '00000громко', '00000точек', '00001',\n",
       "       '00007', '0001', '0002', '000доктора', '000какой',\n",
       "       '000косметические', '000р', '000теряются', '001', '002', '003',\n",
       "       '00381', '006', '007', '00в', '00вых', '00е', '00м', '00по', '00с',\n",
       "       '00седьмого', '00х', '00ые', '00ых', '01', '011', '013'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8cd424-ff6c-45f6-b850-75c0d60e29b3",
   "metadata": {},
   "source": [
    "We can see that if we do not limit the vocabulary, we will have very infrequent words, so we better do it.  \n",
    "For that we have to choose the `min_df` and `max_df` thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18029f-82f1-43d4-abec-63f1aa638aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 39.3 s\n",
      "Wall time: 39.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(186063, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=0.8)\n",
    "vectors = vectorizer.fit_transform(train[\"review\"])\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216d5d9-6b3e-4361-82e2-cdd1aca7b9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['как', 'на', 'не', 'но', 'то', 'что', 'это'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a739c2-1ef0-4ed8-8f69-10f3c5bf0955",
   "metadata": {},
   "source": [
    "These words are in the 80% of all reviews, and it is understandable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5ba31-67f2-419a-aa75-55b1a00da3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 3284) with lowercase turned on and min_df=0.01\n",
      "CPU times: total: 39.4 s\n",
      "Wall time: 39.4 s\n"
     ]
    }
   ],
   "source": [
    "MIN_DF = 0.01\n",
    "vectorizer = CountVectorizer(min_df=MIN_DF)\n",
    "vectors = vectorizer.fit_transform(train[\"review\"])\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors.shape} with lowercase turned on and min_df={MIN_DF}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23028114-901c-403d-9d21-18a2e8046447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '100', '11', '12', '13', '15', '16', '18', '20', '2012',\n",
       "       '21', '30', '3d', '40', '50', '60', '70', '80', '90', 'dc',\n",
       "       'marvel', 'of', 'the', 'абсолютно', 'аватар', 'автор', 'автора',\n",
       "       'авторов', 'авторы', 'аж', 'актер', 'актера', 'актерам',\n",
       "       'актерами', 'актерах', 'актеров', 'актером', 'актерская',\n",
       "       'актерский', 'актерского', 'актерской', 'актерскую', 'актеры',\n",
       "       'актриса', 'актрисы', 'актёр', 'актёра', 'актёров', 'актёрская',\n",
       "       'актёрский'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47c2a4-e7bb-4d12-9fa0-a7e65f89a714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 3281) with lowercase turned on and min_df=0.01 and max_df=0.9\n",
      "CPU times: total: 39.2 s\n",
      "Wall time: 39.2 s\n"
     ]
    }
   ],
   "source": [
    "MIN_DF = 0.01\n",
    "MAX_DF = 0.9\n",
    "vectorizer = CountVectorizer(min_df=MIN_DF, max_df=MAX_DF)\n",
    "vectors = vectorizer.fit_transform(train[\"review\"])\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors.shape} with lowercase turned on and min_df={MIN_DF} and max_df={MAX_DF}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e449c-35a2-4623-af49-56b00224314b",
   "metadata": {},
   "source": [
    "##### `ngram_range`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418dca24-944e-4d14-81cc-c1b4782366a1",
   "metadata": {},
   "source": [
    "The lower and upper boundary of the range of n-values for different n-grams to be extracted.  \n",
    "All values of n such that min_n ≤ n ≤ max_n will be used.   \n",
    "\n",
    "For example a `ngram_range` of `(1, 1)` means only `unigrams`, `(1, 2)` means `unigrams` and `bigrams`, and `(2, 2)` means only `bigrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041bfa82-cd83-4dd7-812d-843c9db1ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 3281) with lowercase turned on and min_df=0.01 and ngram_range=(1, 3)\n",
      "CPU times: total: 5min 35s\n",
      "Wall time: 7min 7s\n"
     ]
    }
   ],
   "source": [
    "NGRAM_RANGE = (1, 3)\n",
    "vectorizer = CountVectorizer(ngram_range=NGRAM_RANGE, min_df=MIN_DF)\n",
    "train_vectors = vectorizer.fit_transform(train[\"review\"])\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors.shape} with lowercase turned on and min_df={MIN_DF} and ngram_range={NGRAM_RANGE}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6bf59-3c69-44f4-a457-315d44e5a0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '10 лет', '100', '11', '12', '13', '15', '16', '18', '20',\n",
       "       '2012', '21', '30', '3d', '40', '50', '60', '70', '80', '90', 'dc',\n",
       "       'marvel', 'of', 'the', 'абсолютно', 'абсолютно все',\n",
       "       'абсолютно не', 'аватар', 'автор', 'автора', 'авторов', 'авторы',\n",
       "       'аж', 'актер', 'актера', 'актерам', 'актерами', 'актерах',\n",
       "       'актеров', 'актером', 'актерская', 'актерская игра', 'актерский',\n",
       "       'актерский состав', 'актерского', 'актерской', 'актерской игры',\n",
       "       'актерскую', 'актерскую игру', 'актеры'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f18e3-2fb8-4837-998e-54ad6ab946c0",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e57d9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2499eb9e-0431-4d2b-979e-846520568b49",
   "metadata": {},
   "source": [
    "## Vectorizing reviews with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e95e4883-0b3e-401c-9532-33067f4c7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_params = {\n",
    "    \"min_df\": 0.01,\n",
    "    \"ngram_range\": (1, 2),\n",
    "    \"max_features\": 10_000,\n",
    "}\n",
    "\n",
    "review_vectorizer = TfidfVectorizer(**vectorizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "129ba4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages.append((\"vectorizer\", review_vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f292e-a226-4411-96c0-32373008a778",
   "metadata": {},
   "source": [
    "## LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "638c9309-3f0a-4040-ba14-8f3734e0700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(\n",
    "    C=1, random_state=SEED, n_jobs=-1, solver=\"saga\", max_iter=10_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ed1b232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages.append((\"classifier\", log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1080d6-1746-4128-8d5c-db4eb550b65c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1ddb3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1c2f68af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(max_features=10000, min_df=0.01,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1, max_iter=10000, n_jobs=-1,\n",
       "                                    random_state=42, solver='saga'))])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb393be-2162-4f5d-94cf-d7df1dd3cd3e",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "19721bdc-505e-45c0-a0c5-e2a20489d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d9328203-4aad-4fcf-b9b9-b447a5f55c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaging = \"micro\"\n",
    "f1 = f1_score(y_test, pred_labels, average=averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2b9637b4-59ba-4c5c-9394-7bbbe9f34321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with micro-averaging is 0.807\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 score with {averaging}-averaging is {f1.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ed1ef",
   "metadata": {},
   "source": [
    "## Saving pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e5ba7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_path = os.path.join(SAVED_MODELS_PATH, \"TfIdfLogRegSentiment.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4494a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import StringTensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "af3f1829",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_type = [('input', StringTensorType([None, 1]))]\n",
    "seps = {\n",
    "    TfidfVectorizer: {\n",
    "        \"separators\": [\n",
    "            ' ', '.', '\\\\?', ',', ';', ':', '!',\n",
    "            '\\\\(', '\\\\)', '\\n', '\"', \"'\",\n",
    "            \"-\", \"\\\\[\", \"\\\\]\", \"@\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "model_onnx = convert_sklearn(\n",
    "    pipe, \"tfidf\",\n",
    "    initial_types=initial_type,\n",
    "    options=seps, \n",
    "    target_opset=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "81930858",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pipe_path, \"wb\") as f:\n",
    "    f.write(model_onnx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa50ee0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "73d69d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e6000172",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession(pipe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "984c289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fe85723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'input': [[input] for input in X_test]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bd599b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_onx = sess.run(None, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d9328203-4aad-4fcf-b9b9-b447a5f55c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaging = \"micro\"\n",
    "f1 = f1_score(y_test, pred_onx[0], average=averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2b9637b4-59ba-4c5c-9394-7bbbe9f34321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with micro-averaging is 0.72\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 score with {averaging}-averaging is {f1.round(3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2fc695378595f3e48db11f69abbf7a44c0bf6d3915fc4cddfa1186eec08a80f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
