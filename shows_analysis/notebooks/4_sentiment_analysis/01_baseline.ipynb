{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3fb8fba-71a1-491b-aed9-a8614321bffb",
   "metadata": {},
   "source": [
    "# Baseline: Logistic Regression + TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25248dc1-1880-4a5b-a625-4016f7a71726",
   "metadata": {},
   "source": [
    "In this notebook I'm going to create a strong baseline using classical algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b93071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install skl2onnx==1.12.0 onnxruntime==1.13.1 protobuf==3.20.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc808ce3-10b0-4461-a7a8-afefda224190",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9128d-93fe-4eb8-a623-291490a5c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import StringTensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6c20d-f2ed-45eb-9918-bdb8f3bac462",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7281466-2ad2-4b0a-a465-55cb89f72b07",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODELS_PATH = \"saved_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ea325-b4e9-4ddc-9004-028b09f079ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = os.path.join(\"../../../\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f2efd-9f1e-492c-b27a-c81c6120839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_data_path = os.path.join(relative_path, \"3_sentiment_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(SAVED_MODELS_PATH).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d535bd26-22b2-4daa-bc7b-dc41c4bc0693",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2de282-70a1-46a9-b972-b9e42ab3d82d",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f1853-797e-45d9-afbe-23cd1ab035c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 206537 entries, 0 to 206536\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype   \n",
      "---  ------     --------------   -----   \n",
      " 0   sentiment  206537 non-null  category\n",
      " 1   review     206537 non-null  object  \n",
      " 2   fold       206537 non-null  object  \n",
      "dtypes: category(1), object(2)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_parquet(\n",
    "    os.path.join(sentiment_analysis_data_path, \"split_reviews.parquet\")\n",
    ")\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa80db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reviews[reviews[\"fold\"] == \"train\"]\n",
    "test = reviews[reviews[\"fold\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b253462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28954/3246664530.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"review\"] = test[\"review\"].str.replace(\"<p>\", \" \")\n",
      "/tmp/ipykernel_28954/3246664530.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"review\"] = train[\"review\"].str.replace(\"<p>\", \" \")\n"
     ]
    }
   ],
   "source": [
    "test[\"review\"] = test[\"review\"].str.replace(\"<p>\", \" \")\n",
    "train[\"review\"] = train[\"review\"].str.replace(\"<p>\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcc292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185883, 20654, 185883, 20654)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = (\n",
    "    train[\"review\"].values.tolist(),\n",
    "    test[\"review\"].values.tolist(),\n",
    "    train[\"sentiment\"].values.tolist(),\n",
    "    test[\"sentiment\"].values.tolist(),\n",
    ")\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da74bc91-f69e-4336-b274-5abe1c206882",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977cc5fa-10f7-4825-b37c-b5da878cc00a",
   "metadata": {},
   "source": [
    "### Text encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d9208-0200-4f3a-8141-efe83bfde594",
   "metadata": {},
   "source": [
    "For baseline model, I've decided to start with TF-IDF and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009baa8b-c61b-44ef-9d54-91528ef72626",
   "metadata": {},
   "source": [
    "#### Hyperparameter Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2eb9fc-972b-4b52-bce8-fec7f80f9fbd",
   "metadata": {},
   "source": [
    "##### `lowercase`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b415f-c116-4c86-8e6a-75ce62415c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=False)\n",
    "vectors_wo_lowercase = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors_wo_lowercase.shape} with lowercase turned off\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7bcc1d-6c1b-4d35-837f-6d4ec1736002",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectors_w_lowercase = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors_w_lowercase.shape} with lowercase turned on\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac53777-9510-4639-ac9e-c4d48980bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_wo_lowercase.shape[1] - vectors_w_lowercase.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e8b121-307e-45a1-8257-3a9b953d737a",
   "metadata": {},
   "source": [
    "The difference in vocabulary size without making all characters lowercase and with lowercase is more than 100 000, so we better stick to lowercase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c0114-4b5a-4aee-a8ab-11c007e4f2f7",
   "metadata": {},
   "source": [
    "##### `max_df` and `min_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f8394-d780-4991-be85-e555352b6395",
   "metadata": {},
   "source": [
    "`min_df` is used for removing terms that appear **too infrequently**. For example:\n",
    "\n",
    " - `min_df = 0.01` means \"ignore terms that appear in **less than 1% of the documents**\".\n",
    " - `min_df = 5` means \"ignore terms that appear in **less than 5 documents**\".  \n",
    " \n",
    "The default `min_df` is `1`, which means \"ignore terms that appear in **less than 1 document**\".  \n",
    "Thus, the default setting does not ignore any terms.\n",
    "\n",
    "`max_df` is used for removing terms that appear **too frequently**, also known as \"corpus-specific stop words\". For example:\n",
    "\n",
    " - `max_df = 0.50` means \"ignore terms that appear in **more than 50% of the documents**\".\n",
    " - `max_df = 25` means \"ignore terms that appear in **more than 25 documents**\".  \n",
    " \n",
    "The default `max_df` is `1.0`, which means \"ignore terms that appear in **more than 100% of the documents**\".  \n",
    "Thus, the default setting does not ignore any terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9639aed-f80f-4623-95fa-17cfbb9a6459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000', '00000', '000000',\n",
       "       '000000000000000000попкорн000000000000', '000000000000001',\n",
       "       '000000000000на', '00000000000во', '00000000000данной',\n",
       "       '00000000000есть000000000000000',\n",
       "       '00000000000есть000000000000000000', '0000000000жевать',\n",
       "       '0000000000ненавижу00000000', '00000000016', '000000000надо',\n",
       "       '000000000разговаривать0000000000', '00000000визуальная',\n",
       "       '00000001', '000001', '00000громко', '00000точек', '00001',\n",
       "       '00007', '0001', '0002', '000доктора', '000какой',\n",
       "       '000косметические', '000р', '000теряются', '001', '002', '003',\n",
       "       '00381', '006', '007', '00в', '00вых', '00е', '00м', '00по', '00с',\n",
       "       '00седьмого', '00х', '00ые', '00ых', '01', '011', '013'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735a9bd-2ec1-43ad-8a2c-c9744eb4fe13",
   "metadata": {},
   "source": [
    "We can see that if we do not limit the vocabulary, we will have very infrequent words, so we better do it.  \n",
    "For that we have to choose the `min_df` and `max_df` thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18029f-82f1-43d4-abec-63f1aa638aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 39.3 s\n",
      "Wall time: 39.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(186063, 7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=0.8)\n",
    "vectors = vectorizer.fit_transform(X_train)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216d5d9-6b3e-4361-82e2-cdd1aca7b9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['как', 'на', 'не', 'но', 'то', 'что', 'это'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6851ca7f-0cf2-4d8e-b91b-9d6a34d738ce",
   "metadata": {},
   "source": [
    "These words are in the 80% of all reviews, and it is understandable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5ba31-67f2-419a-aa75-55b1a00da3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 3284) with lowercase turned on and min_df=0.01\n",
      "CPU times: total: 39.4 s\n",
      "Wall time: 39.4 s\n"
     ]
    }
   ],
   "source": [
    "MIN_DF = 0.01\n",
    "vectorizer = CountVectorizer(min_df=MIN_DF)\n",
    "vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors.shape} with lowercase turned on and min_df={MIN_DF}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23028114-901c-403d-9d21-18a2e8046447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '100', '11', '12', '13', '15', '16', '18', '20', '2012',\n",
       "       '21', '30', '3d', '40', '50', '60', '70', '80', '90', 'dc',\n",
       "       'marvel', 'of', 'the', 'абсолютно', 'аватар', 'автор', 'автора',\n",
       "       'авторов', 'авторы', 'аж', 'актер', 'актера', 'актерам',\n",
       "       'актерами', 'актерах', 'актеров', 'актером', 'актерская',\n",
       "       'актерский', 'актерского', 'актерской', 'актерскую', 'актеры',\n",
       "       'актриса', 'актрисы', 'актёр', 'актёра', 'актёров', 'актёрская',\n",
       "       'актёрский'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47c2a4-e7bb-4d12-9fa0-a7e65f89a714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 3281) with lowercase turned on and min_df=0.01 and max_df=0.9\n",
      "CPU times: total: 39.2 s\n",
      "Wall time: 39.2 s\n"
     ]
    }
   ],
   "source": [
    "MIN_DF = 0.01\n",
    "MAX_DF = 0.9\n",
    "vectorizer = CountVectorizer(min_df=MIN_DF, max_df=MAX_DF)\n",
    "vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors.shape} with lowercase turned on and min_df={MIN_DF} and max_df={MAX_DF}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087d5686-e6b5-4b23-895a-30fa4ac595d9",
   "metadata": {},
   "source": [
    "##### `ngram_range`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a3a978-bd48-4521-b73b-bed4d1a9df07",
   "metadata": {},
   "source": [
    "The lower and upper boundary of the range of n-values for different n-grams to be extracted.  \n",
    "All values of n such that min_n ≤ n ≤ max_n will be used.   \n",
    "\n",
    "For example a `ngram_range` of `(1, 1)` means only `unigrams`, `(1, 2)` means `unigrams` and `bigrams`, and `(2, 2)` means only `bigrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041bfa82-cd83-4dd7-812d-843c9db1ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 3281) with lowercase turned on and min_df=0.01 and ngram_range=(1, 3)\n",
      "CPU times: total: 5min 35s\n",
      "Wall time: 7min 7s\n"
     ]
    }
   ],
   "source": [
    "NGRAM_RANGE = (1, 3)\n",
    "vectorizer = CountVectorizer(ngram_range=NGRAM_RANGE, min_df=MIN_DF)\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors.shape} with lowercase turned on and min_df={MIN_DF} and ngram_range={NGRAM_RANGE}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6bf59-3c69-44f4-a457-315d44e5a0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '10 лет', '100', '11', '12', '13', '15', '16', '18', '20',\n",
       "       '2012', '21', '30', '3d', '40', '50', '60', '70', '80', '90', 'dc',\n",
       "       'marvel', 'of', 'the', 'абсолютно', 'абсолютно все',\n",
       "       'абсолютно не', 'аватар', 'автор', 'автора', 'авторов', 'авторы',\n",
       "       'аж', 'актер', 'актера', 'актерам', 'актерами', 'актерах',\n",
       "       'актеров', 'актером', 'актерская', 'актерская игра', 'актерский',\n",
       "       'актерский состав', 'актерского', 'актерской', 'актерской игры',\n",
       "       'актерскую', 'актерскую игру', 'актеры'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ff7200-c66a-471b-9f4b-e7843c6f59fc",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514239d5-58d2-479a-b92e-7940aa79d8fa",
   "metadata": {},
   "source": [
    "## Vectorizing reviews with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e4883-0b3e-401c-9532-33067f4c7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_params = {\n",
    "    \"min_df\": 0.01,\n",
    "    \"ngram_range\": (1, 2),\n",
    "    \"max_features\": 10_000,\n",
    "}\n",
    "\n",
    "review_vectorizer = TfidfVectorizer(**vectorizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ba4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages.append((\"vectorizer\", review_vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac07c3fc-c555-4e02-95a6-7530efd0c595",
   "metadata": {},
   "source": [
    "## LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c9309-3f0a-4040-ba14-8f3734e0700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(\n",
    "    C=1, random_state=SEED, n_jobs=-1, solver=\"saga\", max_iter=10_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages.append((\"classifier\", log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d057643-98f6-4cde-a1a6-d3bc7dfa4567",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f68af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(max_features=10000, min_df=0.01,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1, max_iter=10000, n_jobs=-1,\n",
       "                                    random_state=42, solver='saga'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17824baf-75f8-437b-8abb-00c989b42f54",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19721bdc-505e-45c0-a0c5-e2a20489d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9328203-4aad-4fcf-b9b9-b447a5f55c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaging = \"micro\"\n",
    "f1 = f1_score(y_test, pred_labels, average=averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9637b4-59ba-4c5c-9394-7bbbe9f34321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with micro-averaging is 0.801\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 score with {averaging}-averaging is {f1.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363826c9-5a61-4d16-b181-8dd2f2916719",
   "metadata": {},
   "source": [
    "# ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d59a32-2add-4938-9f30-64eaf4b8dbd7",
   "metadata": {},
   "source": [
    "## Converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_path = os.path.join(SAVED_MODELS_PATH, \"TfIdfLogRegSentiment.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e38c08-019a-4b86-96c5-b5cd972afe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_type = [('input', StringTensorType([None, 1]))]\n",
    "seps = {\n",
    "    TfidfVectorizer: {\n",
    "        \"separators\": [\n",
    "            ' ', '.', '\\\\?', ',', ';', ':', '!',\n",
    "            '\\\\(', '\\\\)', '\\n', '\"', \"'\",\n",
    "            \"-\", \"\\\\[\", \"\\\\]\", \"@\"\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f1829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/extremesarova/projects/shows_analysis/venv/lib/python3.8/site-packages/skl2onnx/common/_container.py:695: UserWarning: Unable to find operator 'Tokenizer' in domain 'com.microsoft' in ONNX, op_version is forced to 1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_onnx = convert_sklearn(\n",
    "    pipe, \"tfidf\",\n",
    "    initial_types=initial_type,\n",
    "    options=seps, \n",
    "    target_opset=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f859e09-fcb6-44ad-862c-84da4e9ab9e8",
   "metadata": {},
   "source": [
    "## Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81930858",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pipe_path, \"wb\") as f:\n",
    "    f.write(model_onnx.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e6e30-2c61-4f0a-9f58-96c4bb5b58c6",
   "metadata": {},
   "source": [
    "## Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6000172",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession(pipe_path)\n",
    "\n",
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[0].name\n",
    "\n",
    "inputs = {'input': [[input] for input in X_test]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd599b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_onx = sess.run(None, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e00dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaging = \"micro\"\n",
    "f1 = f1_score(y_test, pred_onx[0], average=averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c063a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with micro-averaging is 0.72\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 score with {averaging}-averaging is {f1.round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabb45b5-5abf-4758-a8d2-763f6a463e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie-reviews",
   "language": "python",
   "name": "movie-reviews"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2fc695378595f3e48db11f69abbf7a44c0bf6d3915fc4cddfa1186eec08a80f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
