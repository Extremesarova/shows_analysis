{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Logistic Regression + TF-IDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'm going to create a strong baseline model using classic ML algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install skl2onnx==1.12.0 onnxruntime==1.13.1 protobuf==3.20.1 optuna==3.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --no-cache-dir git+https://github.com/optuna/optuna"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import optuna\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import StringTensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = os.path.join(\"../../../\", \"data\")\n",
    "sentiment_analysis_data_path = os.path.join(relative_path, \"3_sentiment_analysis\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODELS_PATH = \"saved_models\"\n",
    "Path(SAVED_MODELS_PATH).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_AVERAGING = \"micro\"\n",
    "\n",
    "VERSION = \"0.1.0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 206537 entries, 0 to 206536\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype   \n",
      "---  ------     --------------   -----   \n",
      " 0   sentiment  206537 non-null  category\n",
      " 1   review     206537 non-null  object  \n",
      " 2   fold       206537 non-null  object  \n",
      "dtypes: category(1), object(2)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_parquet(\n",
    "    os.path.join(sentiment_analysis_data_path, \"split_reviews.parquet\")\n",
    ")\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reviews[reviews[\"fold\"] == \"train\"]\n",
    "test = reviews[reviews[\"fold\"] == \"test\"]\n",
    "\n",
    "del reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185883, 20654, 185883, 20654)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = (\n",
    "    train[\"review\"].str.replace(\"<p>\", \" \").values.tolist(),\n",
    "    test[\"review\"].str.replace(\"<p>\", \" \").values.tolist(),\n",
    "    train[\"sentiment\"].values.tolist(),\n",
    "    test[\"sentiment\"].values.tolist(),\n",
    ")\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For baseline model, I've decided to start with TF-IDF and Logistic Regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Investigation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using `TfidfVectorizer` to check different hyperparameters it will be faster to use `CountVectorizer`, because it has almost the same hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2eb9fc-972b-4b52-bce8-fec7f80f9fbd",
   "metadata": {},
   "source": [
    "##### `lowercase`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (185883, 786860) with lowercase turned off\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=False)\n",
    "vectors_wo_lowercase = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors_wo_lowercase.shape} with lowercase turned off\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (185883, 670194) with lowercase turned on\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectors_w_lowercase = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors_w_lowercase.shape} with lowercase turned on\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_wo_lowercase.shape[1] - vectors_w_lowercase.shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in vocabulary size between cased and uncased models is more than 100 000, so we better stick to lowercase "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `max_df` and `min_df`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`min_df` is used for removing terms that appear **too infrequently**. For example:\n",
    "\n",
    " - `min_df = 0.01` means \"ignore terms that appear in **less than 1% of the documents**\".\n",
    " - `min_df = 5` means \"ignore terms that appear in **less than 5 documents**\".  \n",
    " \n",
    "The default `min_df` is `1`, which means \"ignore terms that appear in **less than 1 document**\".  \n",
    "Thus, the default setting does not ignore any terms.\n",
    "\n",
    "`max_df` is used for removing terms that appear **too frequently**, also known as \"corpus-specific stop words\". For example:\n",
    "\n",
    " - `max_df = 0.5` means \"ignore terms that appear in **more than 50% of the documents**\".\n",
    " - `max_df = 25` means \"ignore terms that appear in **more than 25 documents**\".  \n",
    " \n",
    "The default `max_df` is `1.0`, which means \"ignore terms that appear in **more than 100% of the documents**\".  \n",
    "Thus, the default setting does not ignore any terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000', '00000', '000000',\n",
       "       '000000000000000000попкорн000000000000', '000000000000001',\n",
       "       '000000000000на', '00000000000во', '00000000000данной',\n",
       "       '00000000000есть000000000000000',\n",
       "       '00000000000есть000000000000000000', '0000000000жевать',\n",
       "       '0000000000ненавижу00000000', '00000000016', '000000000надо',\n",
       "       '000000000разговаривать0000000000', '00000000визуальная',\n",
       "       '00000001', '00000громко', '00000точек', '00001', '00007', '0001',\n",
       "       '0002', '000доктора', '000какой', '000косметические', '000р',\n",
       "       '000теряются', '001', '002', '003', '00381', '006', '007', '00в',\n",
       "       '00вых', '00м', '00по', '00с', '00х', '00ые', '00ых', '01', '011',\n",
       "       '013', '014', '015', '01минуту'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that if we do not limit the vocabulary, we will have very infrequent words in the corpora, so we better do it.  \n",
    "For that we have to choose the `min_df` and `max_df` thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185883, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=0.8)\n",
    "vectors = vectorizer.fit_transform(X_train)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['как', 'на', 'не', 'но', 'то', 'что', 'это'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words are in 80% of all reviews, and this is understandable, since these are particles, prepositions and conjunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (185883, 3284) with lowercase turned on and min_df=0.01\n"
     ]
    }
   ],
   "source": [
    "MIN_DF = 0.01\n",
    "vectorizer = CountVectorizer(min_df=MIN_DF)\n",
    "vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors.shape} with lowercase turned on and min_df={MIN_DF}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '100', '11', '12', '13', '15', '16', '18', '20', '2012',\n",
       "       '21', '30', '3d', '40', '50', '60', '70', '80', '90', 'dc',\n",
       "       'marvel', 'of', 'the', 'абсолютно', 'аватар', 'автор', 'автора',\n",
       "       'авторов', 'авторы', 'аж', 'актер', 'актера', 'актерам',\n",
       "       'актерами', 'актерах', 'актеров', 'актером', 'актерская',\n",
       "       'актерский', 'актерского', 'актерской', 'актерскую', 'актеры',\n",
       "       'актриса', 'актрисы', 'актёр', 'актёра', 'актёров', 'актёрская',\n",
       "       'актёрский'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's much better. We removed unpopular tokens and reduced the vocabulary from ~670K to 3.3K tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (185883, 3281) with lowercase turned on and min_df=0.01 and max_df=0.9\n"
     ]
    }
   ],
   "source": [
    "MIN_DF = 0.01\n",
    "MAX_DF = 0.9\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=MIN_DF, max_df=MAX_DF)\n",
    "vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors.shape} with lowercase turned on and min_df={MIN_DF} and max_df={MAX_DF}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll leave default value of `max_df` - 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `ngram_range`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower and upper boundary of the range of n-values for different n-grams to be extracted.  \n",
    "All values of n such that min_n ≤ n ≤ max_n will be used.   \n",
    "\n",
    "For example a `ngram_range` of `(1, 1)` means only `unigrams`, `(1, 2)` means `unigrams` and `bigrams`, and `(2, 2)` means only `bigrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (185883, 4616) with lowercase turned on and min_df=0.01 and ngram_range=(1, 2)\n"
     ]
    }
   ],
   "source": [
    "NGRAM_RANGE = (1, 2)\n",
    "MIN_DF = 0.01\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=NGRAM_RANGE, min_df=MIN_DF)\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {train_vectors.shape} with lowercase turned on and min_df={MIN_DF} and ngram_range={NGRAM_RANGE}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '10 лет', '100', '11', '12', '13', '15', '16', '18', '20',\n",
       "       '2012', '21', '30', '3d', '40', '50', '60', '70', '80', '90', 'dc',\n",
       "       'marvel', 'of', 'the', 'абсолютно', 'абсолютно все',\n",
       "       'абсолютно не', 'аватар', 'автор', 'автора', 'авторов', 'авторы',\n",
       "       'аж', 'актер', 'актера', 'актерам', 'актерами', 'актерах',\n",
       "       'актеров', 'актером', 'актерская', 'актерская игра', 'актерский',\n",
       "       'актерский состав', 'актерского', 'актерской', 'актерской игры',\n",
       "       'актерскую', 'актерскую игру', 'актеры'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my experience, usually `(1, 2)` is a good default value for the `ngram_range`. Sometimes `(1, 3)` can be a little bit better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the base model, I always use logistic regression as a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(\n",
    "    C=1, random_state=SEED, n_jobs=-1, solver=\"saga\", max_iter=10_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, max_iter=10000, n_jobs=-1, random_state=42,\n",
       "                   solver='saga')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with micro-averaging is 0.80139\n"
     ]
    }
   ],
   "source": [
    "pred_labels = log_reg.predict(test_vectors)\n",
    "f1 = f1_score(y_test, pred_labels, average=F1_AVERAGING)\n",
    "\n",
    "print(f\"F1 score with {F1_AVERAGING}-averaging is {f1.round(5)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Investigation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Logistic Regression we can tune the regularization strength (`C`), `solver` and `penalty` type.  \n",
    "In this notebook, I will only tune the first one using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = {\n",
    "    \"C\": optuna.distributions.FloatDistribution(1e-7, 10.0, log=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-17 21:00:47,308]\u001b[0m A new study created in memory with name: no-name-2a574eda-b58c-45ea-b3fd-a5b1091764a3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "n_trials = 20\n",
    "\n",
    "for _ in range(n_trials):\n",
    "    trial = study.ask(distributions)  # pass the pre-defined distributions.\n",
    "\n",
    "    # two hyperparameters are already sampled from the pre-defined distributions\n",
    "    C = trial.params[\"C\"]\n",
    "\n",
    "    clf = LogisticRegression(C=C, solver=\"saga\")\n",
    "    clf.fit(train_vectors, y_train)\n",
    "\n",
    "    pred_labels = log_reg.predict(test_vectors)\n",
    "    f1 = f1_score(y_test, pred_labels, average=F1_AVERAGING)\n",
    "\n",
    "    study.tell(trial, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 3.4530392483603893e-07}, 0.8013944030212066)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params, study.best_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the regularization strength (`C`) we realize that there is no point in changing the default value of `C`, because we are getting the same results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make a simple pipeline with two components: `TfidfVectorizer` and `LogisticRegression` to save model in one file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing reviews with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_params = {\n",
    "    \"min_df\": 0.01,\n",
    "    \"ngram_range\": (1, 2),\n",
    "    \"max_features\": 10_000,\n",
    "}\n",
    "\n",
    "review_vectorizer = TfidfVectorizer(**vectorizer_params)\n",
    "\n",
    "stages.append((\"vectorizer\", review_vectorizer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying reviews with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(\n",
    "    C=1, random_state=SEED, n_jobs=-1, solver=\"saga\", max_iter=10_000\n",
    ")\n",
    "\n",
    "stages.append((\"classifier\", log_reg))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(max_features=10000, min_df=0.01,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1, max_iter=10000, n_jobs=-1,\n",
       "                                    random_state=42, solver='saga'))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline(stages)\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with micro-averaging is 0.80139\n"
     ]
    }
   ],
   "source": [
    "pred_labels = pipe.predict(X_test)\n",
    "f1 = f1_score(y_test, pred_labels, average=F1_AVERAGING)\n",
    "\n",
    "print(f\"F1 score with {F1_AVERAGING}-averaging is {f1.round(5)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our pipeline and we can save it.  \n",
    "We have several choices: ONNX, joblib, pickle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_path = os.path.join(SAVED_MODELS_PATH, f\"TfIdfLogRegSentiment-{VERSION}.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/extremesarova/projects/shows_analysis/venv/lib/python3.8/site-packages/skl2onnx/common/_container.py:695: UserWarning: Unable to find operator 'Tokenizer' in domain 'com.microsoft' in ONNX, op_version is forced to 1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "initial_type = [('input', StringTensorType([None, 1]))]\n",
    "seps = {\n",
    "    TfidfVectorizer: {\n",
    "        \"separators\": [\n",
    "            ' ', '.', '\\\\?', ',', ';', ':', '!',\n",
    "            '\\\\(', '\\\\)', '\\n', '\"', \"'\",\n",
    "            \"-\", \"\\\\[\", \"\\\\]\", \"@\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "model_onnx = convert_sklearn(\n",
    "    pipe, \"tfidf\",\n",
    "    initial_types=initial_type,\n",
    "    options=seps, \n",
    "    target_opset=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pipe_path, \"wb\") as f:\n",
    "    f.write(model_onnx.SerializeToString())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession(pipe_path)\n",
    "\n",
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[0].name\n",
    "\n",
    "inputs = {'input': [[input] for input in X_test]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with micro-averaging is 0.72\n"
     ]
    }
   ],
   "source": [
    "pred_onx = sess.run(None, inputs)\n",
    "f1 = f1_score(y_test, pred_onx[0], average=F1_AVERAGING)\n",
    "\n",
    "print(f\"F1 score with {F1_AVERAGING}-averaging is {f1.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see such a big performance drop because ONNX has some problems with conversion of TF-IDF models.  \n",
    "\n",
    "My 'go-to' solution for saving models was ONNX, but this performance deterioration is not worth it. I'll try alternative ways to save the pipeline. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_path = os.path.join(SAVED_MODELS_PATH, f\"TfIdfLogRegSentiment-{VERSION}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pipe_path, \"wb\") as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(max_features=10000, min_df=0.01,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1, max_iter=10000, n_jobs=-1,\n",
       "                                    random_state=42, solver='saga'))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(pipe_path, \"rb\") as f:\n",
    "    loaded_pipe = pickle.load(f)\n",
    "\n",
    "loaded_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with micro-averaging is 0.80139\n"
     ]
    }
   ],
   "source": [
    "pred_labels_loaded = loaded_pipe.predict(X_test)\n",
    "f1_loaded = f1_score(y_test, pred_labels_loaded, average=F1_AVERAGING)\n",
    "\n",
    "print(f\"F1 score with {F1_AVERAGING}-averaging is {f1_loaded.round(5)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, pickle works well, but have some [security & maintainability issues](https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie-reviews",
   "language": "python",
   "name": "movie-reviews"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fc695378595f3e48db11f69abbf7a44c0bf6d3915fc4cddfa1186eec08a80f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
