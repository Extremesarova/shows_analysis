{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c851a7-3419-49b4-b115-1909dd0e1adf",
   "metadata": {},
   "source": [
    "# Baseline: Logistic Regression + TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36631f4-d7e9-4c48-92fe-f2f388a3b324",
   "metadata": {},
   "source": [
    "In this notebook I'm going to create a strong baseline using classical algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b93071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install skl2onnx==1.12.0 onnxruntime==1.13.1 protobuf==3.20.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3bab6-76e5-4c1a-981e-4f1006c86224",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9128d-93fe-4eb8-a623-291490a5c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import StringTensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6c20d-f2ed-45eb-9918-bdb8f3bac462",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc8f767-3a51-460f-869d-985d5027c7e5",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODELS_PATH = \"saved_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ea325-b4e9-4ddc-9004-028b09f079ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = os.path.join(\"../../../\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f2efd-9f1e-492c-b27a-c81c6120839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_data_path = os.path.join(relative_path, \"3_sentiment_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(SAVED_MODELS_PATH).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc6afa4-81c2-4116-95b4-1c1ede52c871",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259da1b8-158b-4a7c-ae33-4c1b377b8da2",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f1853-797e-45d9-afbe-23cd1ab035c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 206537 entries, 0 to 206536\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype   \n",
      "---  ------     --------------   -----   \n",
      " 0   sentiment  206537 non-null  category\n",
      " 1   review     206537 non-null  object  \n",
      " 2   fold       206537 non-null  object  \n",
      "dtypes: category(1), object(2)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_parquet(\n",
    "    os.path.join(sentiment_analysis_data_path, \"split_reviews.parquet\")\n",
    ")\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa80db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reviews[reviews[\"fold\"] == \"train\"]\n",
    "test = reviews[reviews[\"fold\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b253462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28954/3246664530.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"review\"] = test[\"review\"].str.replace(\"<p>\", \" \")\n",
      "/tmp/ipykernel_28954/3246664530.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"review\"] = train[\"review\"].str.replace(\"<p>\", \" \")\n"
     ]
    }
   ],
   "source": [
    "test[\"review\"] = test[\"review\"].str.replace(\"<p>\", \" \")\n",
    "train[\"review\"] = train[\"review\"].str.replace(\"<p>\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcc292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185883, 20654, 185883, 20654)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = (\n",
    "    train[\"review\"].values.tolist(),\n",
    "    test[\"review\"].values.tolist(),\n",
    "    train[\"sentiment\"].values.tolist(),\n",
    "    test[\"sentiment\"].values.tolist(),\n",
    ")\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d27c0-9c0a-4dca-8847-93b29d42b7e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b2255-efab-4def-b111-b485b81ef8f4",
   "metadata": {},
   "source": [
    "### Text encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4840cd-b11e-49b6-a34a-c0d324a72e5a",
   "metadata": {},
   "source": [
    "For baseline model, I've decided to start with TF-IDF and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d28c25b-8866-4ddb-a356-b120c40cb390",
   "metadata": {},
   "source": [
    "#### Hyperparameter Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a9e7d-f32a-40ad-af53-d97386dc7bc4",
   "metadata": {},
   "source": [
    "##### `lowercase`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b415f-c116-4c86-8e6a-75ce62415c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=False)\n",
    "vectors_wo_lowercase = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors_wo_lowercase.shape} with lowercase turned off\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7bcc1d-6c1b-4d35-837f-6d4ec1736002",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectors_w_lowercase = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors_w_lowercase.shape} with lowercase turned on\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac53777-9510-4639-ac9e-c4d48980bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_wo_lowercase.shape[1] - vectors_w_lowercase.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654009f-512d-4374-a222-c41041c716f8",
   "metadata": {},
   "source": [
    "The difference in vocabulary size without making all characters lowercase and with lowercase is more than 100 000, so we better stick to lowercase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b432626-0df6-4b84-a4cf-74068b462185",
   "metadata": {},
   "source": [
    "##### `max_df` and `min_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0025cfd-90bc-4dff-b7d6-a298ae3710f4",
   "metadata": {},
   "source": [
    "`min_df` is used for removing terms that appear **too infrequently**. For example:\n",
    "\n",
    " - `min_df = 0.01` means \"ignore terms that appear in **less than 1% of the documents**\".\n",
    " - `min_df = 5` means \"ignore terms that appear in **less than 5 documents**\".  \n",
    " \n",
    "The default `min_df` is `1`, which means \"ignore terms that appear in **less than 1 document**\".  \n",
    "Thus, the default setting does not ignore any terms.\n",
    "\n",
    "`max_df` is used for removing terms that appear **too frequently**, also known as \"corpus-specific stop words\". For example:\n",
    "\n",
    " - `max_df = 0.50` means \"ignore terms that appear in **more than 50% of the documents**\".\n",
    " - `max_df = 25` means \"ignore terms that appear in **more than 25 documents**\".  \n",
    " \n",
    "The default `max_df` is `1.0`, which means \"ignore terms that appear in **more than 100% of the documents**\".  \n",
    "Thus, the default setting does not ignore any terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9639aed-f80f-4623-95fa-17cfbb9a6459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000', '00000', '000000',\n",
       "       '000000000000000000попкорн000000000000', '000000000000001',\n",
       "       '000000000000на', '00000000000во', '00000000000данной',\n",
       "       '00000000000есть000000000000000',\n",
       "       '00000000000есть000000000000000000', '0000000000жевать',\n",
       "       '0000000000ненавижу00000000', '00000000016', '000000000надо',\n",
       "       '000000000разговаривать0000000000', '00000000визуальная',\n",
       "       '00000001', '000001', '00000громко', '00000точек', '00001',\n",
       "       '00007', '0001', '0002', '000доктора', '000какой',\n",
       "       '000косметические', '000р', '000теряются', '001', '002', '003',\n",
       "       '00381', '006', '007', '00в', '00вых', '00е', '00м', '00по', '00с',\n",
       "       '00седьмого', '00х', '00ые', '00ых', '01', '011', '013'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8cd424-ff6c-45f6-b850-75c0d60e29b3",
   "metadata": {},
   "source": [
    "We can see that if we do not limit the vocabulary, we will have very infrequent words, so we better do it.  \n",
    "For that we have to choose the `min_df` and `max_df` thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18029f-82f1-43d4-abec-63f1aa638aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 39.3 s\n",
      "Wall time: 39.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(186063, 7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=0.8)\n",
    "vectors = vectorizer.fit_transform(X_train)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216d5d9-6b3e-4361-82e2-cdd1aca7b9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['как', 'на', 'не', 'но', 'то', 'что', 'это'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a739c2-1ef0-4ed8-8f69-10f3c5bf0955",
   "metadata": {},
   "source": [
    "These words are in the 80% of all reviews, and it is understandable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5ba31-67f2-419a-aa75-55b1a00da3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 3284) with lowercase turned on and min_df=0.01\n",
      "CPU times: total: 39.4 s\n",
      "Wall time: 39.4 s\n"
     ]
    }
   ],
   "source": [
    "MIN_DF = 0.01\n",
    "vectorizer = CountVectorizer(min_df=MIN_DF)\n",
    "vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors.shape} with lowercase turned on and min_df={MIN_DF}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23028114-901c-403d-9d21-18a2e8046447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '100', '11', '12', '13', '15', '16', '18', '20', '2012',\n",
       "       '21', '30', '3d', '40', '50', '60', '70', '80', '90', 'dc',\n",
       "       'marvel', 'of', 'the', 'абсолютно', 'аватар', 'автор', 'автора',\n",
       "       'авторов', 'авторы', 'аж', 'актер', 'актера', 'актерам',\n",
       "       'актерами', 'актерах', 'актеров', 'актером', 'актерская',\n",
       "       'актерский', 'актерского', 'актерской', 'актерскую', 'актеры',\n",
       "       'актриса', 'актрисы', 'актёр', 'актёра', 'актёров', 'актёрская',\n",
       "       'актёрский'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47c2a4-e7bb-4d12-9fa0-a7e65f89a714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 3281) with lowercase turned on and min_df=0.01 and max_df=0.9\n",
      "CPU times: total: 39.2 s\n",
      "Wall time: 39.2 s\n"
     ]
    }
   ],
   "source": [
    "MIN_DF = 0.01\n",
    "MAX_DF = 0.9\n",
    "vectorizer = CountVectorizer(min_df=MIN_DF, max_df=MAX_DF)\n",
    "vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors.shape} with lowercase turned on and min_df={MIN_DF} and max_df={MAX_DF}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e449c-35a2-4623-af49-56b00224314b",
   "metadata": {},
   "source": [
    "##### `ngram_range`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418dca24-944e-4d14-81cc-c1b4782366a1",
   "metadata": {},
   "source": [
    "The lower and upper boundary of the range of n-values for different n-grams to be extracted.  \n",
    "All values of n such that min_n ≤ n ≤ max_n will be used.   \n",
    "\n",
    "For example a `ngram_range` of `(1, 1)` means only `unigrams`, `(1, 2)` means `unigrams` and `bigrams`, and `(2, 2)` means only `bigrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041bfa82-cd83-4dd7-812d-843c9db1ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 3281) with lowercase turned on and min_df=0.01 and ngram_range=(1, 3)\n",
      "CPU times: total: 5min 35s\n",
      "Wall time: 7min 7s\n"
     ]
    }
   ],
   "source": [
    "NGRAM_RANGE = (1, 3)\n",
    "vectorizer = CountVectorizer(ngram_range=NGRAM_RANGE, min_df=MIN_DF)\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors.shape} with lowercase turned on and min_df={MIN_DF} and ngram_range={NGRAM_RANGE}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6bf59-3c69-44f4-a457-315d44e5a0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '10 лет', '100', '11', '12', '13', '15', '16', '18', '20',\n",
       "       '2012', '21', '30', '3d', '40', '50', '60', '70', '80', '90', 'dc',\n",
       "       'marvel', 'of', 'the', 'абсолютно', 'абсолютно все',\n",
       "       'абсолютно не', 'аватар', 'автор', 'автора', 'авторов', 'авторы',\n",
       "       'аж', 'актер', 'актера', 'актерам', 'актерами', 'актерах',\n",
       "       'актеров', 'актером', 'актерская', 'актерская игра', 'актерский',\n",
       "       'актерский состав', 'актерского', 'актерской', 'актерской игры',\n",
       "       'актерскую', 'актерскую игру', 'актеры'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f18e3-2fb8-4837-998e-54ad6ab946c0",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2499eb9e-0431-4d2b-979e-846520568b49",
   "metadata": {},
   "source": [
    "## Vectorizing reviews with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e4883-0b3e-401c-9532-33067f4c7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_params = {\n",
    "    \"min_df\": 0.01,\n",
    "    \"ngram_range\": (1, 2),\n",
    "    \"max_features\": 10_000,\n",
    "}\n",
    "\n",
    "review_vectorizer = TfidfVectorizer(**vectorizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ba4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages.append((\"vectorizer\", review_vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f292e-a226-4411-96c0-32373008a778",
   "metadata": {},
   "source": [
    "## LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c9309-3f0a-4040-ba14-8f3734e0700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(\n",
    "    C=1, random_state=SEED, n_jobs=-1, solver=\"saga\", max_iter=10_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages.append((\"classifier\", log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1080d6-1746-4128-8d5c-db4eb550b65c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f68af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(max_features=10000, min_df=0.01,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1, max_iter=10000, n_jobs=-1,\n",
       "                                    random_state=42, solver='saga'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb393be-2162-4f5d-94cf-d7df1dd3cd3e",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19721bdc-505e-45c0-a0c5-e2a20489d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9328203-4aad-4fcf-b9b9-b447a5f55c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaging = \"micro\"\n",
    "f1 = f1_score(y_test, pred_labels, average=averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9637b4-59ba-4c5c-9394-7bbbe9f34321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with micro-averaging is 0.801\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 score with {averaging}-averaging is {f1.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ed1ef",
   "metadata": {},
   "source": [
    "# ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a0e39",
   "metadata": {},
   "source": [
    "## Converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_path = os.path.join(SAVED_MODELS_PATH, \"TfIdfLogRegSentiment.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e38c08-019a-4b86-96c5-b5cd972afe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_type = [('input', StringTensorType([None, 1]))]\n",
    "seps = {\n",
    "    TfidfVectorizer: {\n",
    "        \"separators\": [\n",
    "            ' ', '.', '\\\\?', ',', ';', ':', '!',\n",
    "            '\\\\(', '\\\\)', '\\n', '\"', \"'\",\n",
    "            \"-\", \"\\\\[\", \"\\\\]\", \"@\"\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f1829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/extremesarova/projects/shows_analysis/venv/lib/python3.8/site-packages/skl2onnx/common/_container.py:695: UserWarning: Unable to find operator 'Tokenizer' in domain 'com.microsoft' in ONNX, op_version is forced to 1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_onnx = convert_sklearn(\n",
    "    pipe, \"tfidf\",\n",
    "    initial_types=initial_type,\n",
    "    options=seps, \n",
    "    target_opset=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9156db",
   "metadata": {},
   "source": [
    "## Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81930858",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pipe_path, \"wb\") as f:\n",
    "    f.write(model_onnx.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dda567",
   "metadata": {},
   "source": [
    "## Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6000172",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession(pipe_path)\n",
    "\n",
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[0].name\n",
    "\n",
    "inputs = {'input': [[input] for input in X_test]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd599b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_onx = sess.run(None, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e00dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaging = \"micro\"\n",
    "f1 = f1_score(y_test, pred_onx[0], average=averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c063a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with micro-averaging is 0.72\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 score with {averaging}-averaging is {f1.round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabb45b5-5abf-4758-a8d2-763f6a463e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie-reviews",
   "language": "python",
   "name": "movie-reviews"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2fc695378595f3e48db11f69abbf7a44c0bf6d3915fc4cddfa1186eec08a80f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
