{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c851a7-3419-49b4-b115-1909dd0e1adf",
   "metadata": {},
   "source": [
    "# Baseline: Logistic Regression + TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36631f4-d7e9-4c48-92fe-f2f388a3b324",
   "metadata": {},
   "source": [
    "In this notebook I'm going to create a strong baseline using classical algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3bab6-76e5-4c1a-981e-4f1006c86224",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f9128d-93fe-4eb8-a623-291490a5c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import (GridSearchCV, StratifiedKFold,\n",
    "                                     train_test_split)\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e6c20d-f2ed-45eb-9918-bdb8f3bac462",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc8f767-3a51-460f-869d-985d5027c7e5",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d31ea325-b4e9-4ddc-9004-028b09f079ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = os.path.join(\"../../../../\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "589f2efd-9f1e-492c-b27a-c81c6120839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_data_path = os.path.join(relative_path, \"3_sentiment_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e730e218",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6e3128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIND_HYPERPARAMETERS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc6afa4-81c2-4116-95b4-1c1ede52c871",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259da1b8-158b-4a7c-ae33-4c1b377b8da2",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b68f1853-797e-45d9-afbe-23cd1ab035c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 206737 entries, 0 to 206736\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype   \n",
      "---  ------     --------------   -----   \n",
      " 0   sentiment  206737 non-null  category\n",
      " 1   review     206737 non-null  object  \n",
      " 2   fold       206737 non-null  object  \n",
      "dtypes: category(1), object(2)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_parquet(\n",
    "    os.path.join(sentiment_analysis_data_path, \"split_reviews.parquet\")\n",
    ")\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aa80db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reviews[reviews[\"fold\"] == \"train\"]\n",
    "test = reviews[reviews[\"fold\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d27c0-9c0a-4dca-8847-93b29d42b7e5",
   "metadata": {},
   "source": [
    "# Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b2255-efab-4def-b111-b485b81ef8f4",
   "metadata": {},
   "source": [
    "### Text encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4840cd-b11e-49b6-a34a-c0d324a72e5a",
   "metadata": {},
   "source": [
    "For baseline model, I've decided to start with TF-IDF and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d28c25b-8866-4ddb-a356-b120c40cb390",
   "metadata": {},
   "source": [
    "#### Hyperparameter Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a9e7d-f32a-40ad-af53-d97386dc7bc4",
   "metadata": {},
   "source": [
    "##### `lowercase`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad0b415f-c116-4c86-8e6a-75ce62415c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 785942) with lowercase turned off\n",
      "CPU times: total: 40.1 s\n",
      "Wall time: 40.1 s\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=False)\n",
    "vectors_wo_lowercase = vectorizer.fit_transform(train[\"review\"])\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors_wo_lowercase.shape} with lowercase turned off\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d7bcc1d-6c1b-4d35-837f-6d4ec1736002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 669383) with lowercase turned on\n",
      "CPU times: total: 41.8 s\n",
      "Wall time: 41.8 s\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectors_w_lowercase = vectorizer.fit_transform(train[\"review\"])\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors_w_lowercase.shape} with lowercase turned on\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dac53777-9510-4639-ac9e-c4d48980bf9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116559"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_wo_lowercase.shape[1] - vectors_w_lowercase.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654009f-512d-4374-a222-c41041c716f8",
   "metadata": {},
   "source": [
    "The difference in vocabulary size without making all characters lowercase and with lowercase is more than 100 000, so we better stick to lowercase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b432626-0df6-4b84-a4cf-74068b462185",
   "metadata": {},
   "source": [
    "##### `max_df` and `min_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0025cfd-90bc-4dff-b7d6-a298ae3710f4",
   "metadata": {},
   "source": [
    "`min_df` is used for removing terms that appear **too infrequently**. For example:\n",
    "\n",
    " - `min_df = 0.01` means \"ignore terms that appear in **less than 1% of the documents**\".\n",
    " - `min_df = 5` means \"ignore terms that appear in **less than 5 documents**\".  \n",
    " \n",
    "The default `min_df` is `1`, which means \"ignore terms that appear in **less than 1 document**\".  \n",
    "Thus, the default setting does not ignore any terms.\n",
    "\n",
    "`max_df` is used for removing terms that appear **too frequently**, also known as \"corpus-specific stop words\". For example:\n",
    "\n",
    " - `max_df = 0.50` means \"ignore terms that appear in **more than 50% of the documents**\".\n",
    " - `max_df = 25` means \"ignore terms that appear in **more than 25 documents**\".  \n",
    " \n",
    "The default `max_df` is `1.0`, which means \"ignore terms that appear in **more than 100% of the documents**\".  \n",
    "Thus, the default setting does not ignore any terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9639aed-f80f-4623-95fa-17cfbb9a6459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000', '00000', '000000',\n",
       "       '000000000000000000попкорн000000000000', '000000000000001',\n",
       "       '000000000000на', '00000000000во', '00000000000данной',\n",
       "       '00000000000есть000000000000000',\n",
       "       '00000000000есть000000000000000000', '0000000000жевать',\n",
       "       '0000000000ненавижу00000000', '00000000016', '000000000надо',\n",
       "       '000000000разговаривать0000000000', '00000000визуальная',\n",
       "       '00000001', '000001', '00000громко', '00000точек', '00001',\n",
       "       '00007', '0001', '0002', '000доктора', '000какой',\n",
       "       '000косметические', '000р', '000теряются', '001', '002', '003',\n",
       "       '00381', '006', '007', '00в', '00вых', '00е', '00м', '00по', '00с',\n",
       "       '00седьмого', '00х', '00ые', '00ых', '01', '011', '013'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8cd424-ff6c-45f6-b850-75c0d60e29b3",
   "metadata": {},
   "source": [
    "We can see that if we do not limit the vocabulary, we will have very infrequent words, so we better do it.  \n",
    "For that we have to choose the `min_df` and `max_df` thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac18029f-82f1-43d4-abec-63f1aa638aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 39.3 s\n",
      "Wall time: 39.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(186063, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=0.8)\n",
    "vectors = vectorizer.fit_transform(train[\"review\"])\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e216d5d9-6b3e-4361-82e2-cdd1aca7b9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['как', 'на', 'не', 'но', 'то', 'что', 'это'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a739c2-1ef0-4ed8-8f69-10f3c5bf0955",
   "metadata": {},
   "source": [
    "These words are in the 80% of all reviews, and it is understandable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57a5ba31-67f2-419a-aa75-55b1a00da3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 3284) with lowercase turned on and min_df=0.01\n",
      "CPU times: total: 39.4 s\n",
      "Wall time: 39.4 s\n"
     ]
    }
   ],
   "source": [
    "MIN_DF = 0.01\n",
    "vectorizer = CountVectorizer(min_df=MIN_DF)\n",
    "vectors = vectorizer.fit_transform(train[\"review\"])\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors.shape} with lowercase turned on and min_df={MIN_DF}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23028114-901c-403d-9d21-18a2e8046447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '100', '11', '12', '13', '15', '16', '18', '20', '2012',\n",
       "       '21', '30', '3d', '40', '50', '60', '70', '80', '90', 'dc',\n",
       "       'marvel', 'of', 'the', 'абсолютно', 'аватар', 'автор', 'автора',\n",
       "       'авторов', 'авторы', 'аж', 'актер', 'актера', 'актерам',\n",
       "       'актерами', 'актерах', 'актеров', 'актером', 'актерская',\n",
       "       'актерский', 'актерского', 'актерской', 'актерскую', 'актеры',\n",
       "       'актриса', 'актрисы', 'актёр', 'актёра', 'актёров', 'актёрская',\n",
       "       'актёрский'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b47c2a4-e7bb-4d12-9fa0-a7e65f89a714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 3281) with lowercase turned on and min_df=0.01 and max_df=0.9\n",
      "CPU times: total: 39.2 s\n",
      "Wall time: 39.2 s\n"
     ]
    }
   ],
   "source": [
    "MIN_DF = 0.01\n",
    "MAX_DF = 0.9\n",
    "vectorizer = CountVectorizer(min_df=MIN_DF, max_df=MAX_DF)\n",
    "vectors = vectorizer.fit_transform(train[\"review\"])\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors.shape} with lowercase turned on and min_df={MIN_DF} and max_df={MAX_DF}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e449c-35a2-4623-af49-56b00224314b",
   "metadata": {},
   "source": [
    "##### `ngram_range`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418dca24-944e-4d14-81cc-c1b4782366a1",
   "metadata": {},
   "source": [
    "The lower and upper boundary of the range of n-values for different n-grams to be extracted.  \n",
    "All values of n such that min_n ≤ n ≤ max_n will be used.   \n",
    "\n",
    "For example a `ngram_range` of `(1, 1)` means only `unigrams`, `(1, 2)` means `unigrams` and `bigrams`, and `(2, 2)` means only `bigrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "041bfa82-cd83-4dd7-812d-843c9db1ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (186063, 3281) with lowercase turned on and min_df=0.01 and ngram_range=(1, 3)\n",
      "CPU times: total: 5min 35s\n",
      "Wall time: 7min 7s\n"
     ]
    }
   ],
   "source": [
    "NGRAM_RANGE = (1, 3)\n",
    "vectorizer = CountVectorizer(ngram_range=NGRAM_RANGE, min_df=MIN_DF)\n",
    "train_vectors = vectorizer.fit_transform(train[\"review\"])\n",
    "\n",
    "print(\n",
    "    f\"The size of the train dataset is {vectors.shape} with lowercase turned on and min_df={MIN_DF} and ngram_range={NGRAM_RANGE}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09b6bf59-3c69-44f4-a457-315d44e5a0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '10 лет', '100', '11', '12', '13', '15', '16', '18', '20',\n",
       "       '2012', '21', '30', '3d', '40', '50', '60', '70', '80', '90', 'dc',\n",
       "       'marvel', 'of', 'the', 'абсолютно', 'абсолютно все',\n",
       "       'абсолютно не', 'аватар', 'автор', 'автора', 'авторов', 'авторы',\n",
       "       'аж', 'актер', 'актера', 'актерам', 'актерами', 'актерах',\n",
       "       'актеров', 'актером', 'актерская', 'актерская игра', 'актерский',\n",
       "       'актерский состав', 'актерского', 'актерской', 'актерской игры',\n",
       "       'актерскую', 'актерскую игру', 'актеры'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f18e3-2fb8-4837-998e-54ad6ab946c0",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2499eb9e-0431-4d2b-979e-846520568b49",
   "metadata": {},
   "source": [
    "## Vectorizing reviews with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95e4883-0b3e-401c-9532-33067f4c7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_params = {\n",
    "    \"min_df\": 0.01,\n",
    "    \"ngram_range\": (1, 2),\n",
    "    \"max_features\": 10_000,\n",
    "    \"tokenizer\": lambda s: s.split(),\n",
    "}\n",
    "\n",
    "review_vectorizer = TfidfVectorizer(**vectorizer_params)\n",
    "# review_vectorizer = CountVectorizer(**vectorizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c3db587-2d69-4a1b-8660-0076025cf14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_review = review_vectorizer.fit_transform(train[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34e4d213-c5d6-44da-bd0a-27395a3368af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_review = review_vectorizer.transform(test[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b814a402-d97a-482f-8dc8-d1fd527fb381",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6886976e-6039-498d-be0f-5025740f4dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45cceb73-fe4c-425a-8a99-00598cb4db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = le.fit_transform(train[\"sentiment\"])\n",
    "test_labels = le.transform(test[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef2ebe15-555d-43ba-aa92-6e6ae85f2362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((186063,), (20674,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f292e-a226-4411-96c0-32373008a778",
   "metadata": {},
   "source": [
    "## LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1080d6-1746-4128-8d5c-db4eb550b65c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "638c9309-3f0a-4040-ba14-8f3734e0700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(\n",
    "    C=1, random_state=SEED, n_jobs=-1, solver=\"saga\", max_iter=10_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d55c2c9e-5d3e-49cd-bb5f-636a1136467f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, max_iter=10000, n_jobs=-1, random_state=42,\n",
       "                   solver='saga')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(X_train_review, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c0241-a35e-4627-90d2-16e34f9220dc",
   "metadata": {},
   "source": [
    "#### Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a170126a-6054-41ca-b88d-4206cc831d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIND_HYPERPARAMETERS:\n",
    "    parameters = {\"C\": np.logspace(-2, 2, 50)}\n",
    "\n",
    "    clf = GridSearchCV(log_reg, parameters, n_jobs=-1, cv=StratifiedKFold(2))\n",
    "    clf.fit(X_train_review, train_labels)\n",
    "\n",
    "    clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35867e5d-3e52-4722-9492-b5ba215bc650",
   "metadata": {},
   "source": [
    "Hyperparameter tuning doesn't really improve the score, so I'll stick to default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb393be-2162-4f5d-94cf-d7df1dd3cd3e",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19721bdc-505e-45c0-a0c5-e2a20489d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = log_reg.predict(X_test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9328203-4aad-4fcf-b9b9-b447a5f55c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaging = \"micro\"\n",
    "f1 = f1_score(test_labels, pred_labels, average=averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b9637b4-59ba-4c5c-9394-7bbbe9f34321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with micro-averaging is 0.793\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 score with {averaging}-averaging is {f1.round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b54683-7182-457c-b8af-21192395d011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie_reviews",
   "language": "python",
   "name": "movie_reviews"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
